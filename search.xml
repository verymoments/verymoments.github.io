<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BIOS&amp;UEFI启动流程</title>
    <url>/2022/05/09/BIOS-UEFI%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<h1><span id="基础知识">基础知识</span></h1><h2><span id="mbr">MBR</span></h2><p><em><strong>主引导记录</strong></em>（Master Boot Record，缩写：MBR），又叫做主引导扇区，是计算机开机后访问硬盘时所必须要读取的首个扇区，它在硬盘上的三维地址为（柱面，磁头，扇区）＝（0，0，1）。在深入讨论主引导扇区内部结构的时候，有时也将其开头的446字节内容特指为“主引导记录”（MBR），其后是4个16字节的“磁盘分区表”（DPT），以及2字节的结束标志（55AA）。因此，在使用“主引导记录”（MBR）这个术语的时候，需要根据具体情况判断其到底是指整个主引导扇区，还是主引导扇区的前446字节。</p>
<p>主引导扇区记录着硬盘本身的相关信息以及硬盘各个分区的大小及位置信息，是数据信息的重要入口。如果它受到破坏，硬盘上的基本数据结构信息将会丢失，需要用繁琐的方式试探性的重建数据结构信息后才可能重新访问原先的数据。主引导扇区内的信息可以通过任何一种基于某种操作系统的分区工具软件写入，但和某种操作系统没有特定的关系，即只要创建了有效的主引导记录就可以引导任意一种操作系统（操作系统是创建在高级格式化的硬盘分区之上，是和一定的文件系统相联系的）。</p>
<p>对于硬盘而言，一个扇区可能的字节数为128×2n（n&#x3D;0,1,2,3）。大多情况下，取n&#x3D;2，即一个扇区（sector）的大小为512字节</p>
<span id="more"></span>

<h3><span id="启动代码">启动代码</span></h3><p>主引导记录最开头是第一阶段引导代码。其中的硬盘引导程序的主要作用是检查分区表是否正确并且在系统硬件完成自检以后将控制权交给硬盘上的引导程序（如GNU GRUB）。它不依赖任何操作系统，而且启动代码也是可以改变的，从而能够实现多系统引导。</p>
<h3><span id="硬盘分区表">硬盘分区表</span></h3><p>硬盘分区表占据主引导扇区的64个字节（偏移01BEH–偏移01FDH），可以对<em><strong>四个分区</strong></em>的信息进行描述，其中每个分区的信息占据16个字节</p>
<h3><span id="主引导扇区的读取流程">主引导扇区的读取流程</span></h3><ol>
<li>BIOS加电自检,BIOS执行内存地址为FFFF:0000H处的跳转指令，跳转到固化在ROM中的自检程序处，对系统硬件（包括内存）进行检查。</li>
<li>读取主引导记录（MBR）。当BIOS检查到硬件正常并与CMOS中的设置相符后，按照CMOS中对启动设备的设置顺序检测可用的启动设备。BIOS将相应启动设备的第一个扇区（也就是MBR扇区）读入内存地址为0000:7C00H处。</li>
<li>检查0000:01FEH-0000:01FFH（MBR的结束标志位）是否等于55AAH，若不等于则转去尝试其他启动设备，如果没有启动设备满足要求则显示”NO ROM BASIC”然后死机。</li>
<li>当检测到有启动设备满足要求后，BIOS将控制权交给相应启动设备。启动设备的MBR将自己复制到0000:0600H处，然后继续执行。</li>
<li>根据MBR中的引导代码启动引导程序。</li>
</ol>
<h2><span id="gpt">GPT</span></h2><p>GUID Partition Table，缩写：GPT,全局唯一表示分区表，是一个实体硬盘的分区表的结构布局的标准。它是可扩展固件接口（UEFI）标准（被Intel用于替代个人计算机的BIOS）的一部分，被用于替代BIOS系统中的一32bits来存储逻辑块地址和大小信息的主引导记录（MBR）分区表。对于那些扇区为512字节的磁盘，MBR分区表不支持容量大于2.2TB（2.2×1012字节）的分区，然而，一些硬盘制造商（诸如希捷和西部数据）注意到这个局限性，并且将他们的容量较大的磁盘升级到4KB的扇区，这意味着MBR的有效容量上限提升到16 TiB。 与MBR分区的磁盘不同，至关重要的平台操作数据位于分区，而不是位于非分区或隐藏扇区。另外，GPT分区磁盘有备份分区表来提高分区数据结构的完整性。在UEFI系统上，通常是通过ESP分区中的EFI应用程序文件启动GPT硬盘上的操作系统，而不是活动主分区上的引导程序。</p>
<h3><span id="特点">特点</span></h3><p>在MBR硬盘中，分区信息直接存储于主引导记录（MBR）中（主引导记录中还存储着系统的引导程序）。但在GPT硬盘中，分区表的位置信息储存在GPT头中。但出于兼容性考虑，硬盘的第一个扇区仍然用作MBR，之后才是GPT头。 跟现代的MBR一样，GPT也使用逻辑区块地址（LBA）取代了早期的CHS寻址方式。传统MBR信息存储于LBA 0，GPT头存储于LBA 1，接下来才是分区表本身。64位Windows操作系统使用16,384字节（或32扇区）作为GPT分区表，接下来的LBA 34是硬盘上第一个分区的开始。</p>
<p><em><strong>为了减少分区表损坏的风险，GPT在硬盘最后保存了一份分区表的副本。</strong></em></p>
<h3><span id="传统的mbr">传统的MBR</span></h3><p>在GPT分区表的最开头，出于兼容性考虑仍然存储了一份传统的MBR，用来防止不支持GPT的硬盘管理工具错误识别并破坏硬盘中的数据，这个MBR也叫做保护MBR。在支持从GPT启动的操作系统中，这里也用于存储第一阶段的启动代码。在这个MBR中，只有一个标识为0xEE的分区，以此来表示这块硬盘使用GPT分区表。不能识别GPT硬盘的操作系统通常会识别出一个未知类型的分区，并且拒绝对硬盘进行操作，除非用户特别要求删除这个分区。这就避免了意外删除分区的危险。另外，能够识别GPT分区表的操作系统会检查保护MBR中的分区表，如果分区类型不是0xEE或者MBR分区表中有多个项，也会拒绝对硬盘进行操作。</p>
<p>在使用MBR&#x2F;GPT混合分区表的硬盘中，这部分存储了GPT分区表的一部分分区（通常是前四个分区），可以使不支持从GPT启动的操作系统从这个MBR启动，启动后只能操作MBR分区表中的分区。如Boot Camp就是使用这种方式启动Windows</p>
<h3><span id="分区表头lba-1">分区表头（LBA 1）</span></h3><p>分区表头定义了硬盘的可用空间以及组成分区表的项的大小和数量。</p>
<p>分区表头还记录了这块硬盘的GUID，记录了分区表头本身的位置和大小（位置总是在LBA1）以及备份分区表头和分区表的位置和大小（在硬盘的最后）。它还储存着它本身和分区表的CRC32校验。固件、引导程序和操作系统在启动时可以根据这个校验值来判断分区表是否出错，如果出错了，可以使用软件从硬盘最后的备份GPT中恢复整个分区表，如果备份GPT也校验错误，硬盘将不可使用。所以GPT硬盘的分区表不可以直接使用16进制编辑器修改。</p>
<p><em><strong>主分区表和备份分区表的头分别位于硬盘的第二个扇区（LBA1）以及硬盘的最后一个扇区。备份分区表头中的信息是关于备份分区表的。</strong></em></p>
<h3><span id="分区表项lba-233">分区表项（LBA 2–33）</span></h3><p>GPT分区表使用简单而直接的方式表示分区。一个分区表项的前16字节是分区类型GUID。例如，EFI系统分区的GUID类型是{C12A7328-F81F-11D2-BA4B-00A0C93EC93B}。接下来的16字节是该分区唯一的GUID（这个GUID指的是该分区本身，而之前的GUID指的是该分区的类型）。再接下来是分区起始和末尾的64位LBA编号，以及分区的名字和属性。</p>
<h1><span id="bios启动流程">BIOS启动流程</span></h1><ol>
<li>System switched on, the power-on self-test (POST) is executed.</li>
<li>After POST, BIOS initializes the necessary system hardware for booting (disk, keyboard controllers etc.).</li>
<li>BIOS launches the first 440 bytes (the Master Boot Record bootstrap code area) of the first disk in the BIOS disk order.</li>
<li>The boot loader’s first stage in the MBR boot code then launches its second stage code (if any) from either:<ul>
<li>next disk sectors after the MBR, i.e. the so called post-MBR gap (only on a MBR partition table).</li>
<li>a partition’s or a partitionless disk’s volume boot record (VBR)</li>
<li>the BIOS boot partition (GRUB on BIOS&#x2F;GPT only).</li>
</ul>
</li>
<li>The actual boot loader is launched.</li>
<li>The boot loader then loads an operating system by either chain-loading or directly loading the operating system kernel.</li>
</ol>
<h1><span id="uefi启动流程">UEFI启动流程</span></h1><ol>
<li><p>System switched on, the power-on self-test (POST) is executed.</p>
</li>
<li><p>After POST, UEFI initializes the hardware required for booting (disk, keyboard controllers etc.).</p>
</li>
<li><p>Firmware reads the boot entries in the NVRAM to determine which EFI application to launch and from where (e.g. from which disk and partition).</p>
<ul>
<li>A boot entry could simply be a disk. In this case the firmware looks for an EFI system partition on that disk and tries to find an EFI application in the fallback boot path \EFI\BOOT\BOOTx64.EFI (BOOTIA32.EFI on systems with a IA32 (32-bit) UEFI). This is how UEFI bootable removable media work.</li>
</ul>
<blockquote>
<p><em><strong>NVRAM Non-Volatile Random Access Memory,是非易失性随机访问存储器，指断电后仍能保持数据的一种RAM，对于uefi中NVRAM则是存储在固件ROM中的一段位置。</strong></em></p>
</blockquote>
</li>
<li><p>Firmware launches the EFI application.</p>
<ul>
<li>This could be a boot loader or the Arch kernel itself using EFISTUB.</li>
<li>It could be some other EFI application such as a UEFI shell or a boot manager like systemd-boot or rEFInd.</li>
</ul>
</li>
</ol>
<h1><span id="grub2启动流程">GRUB2启动流程</span></h1><h2><span id="bios中的启动流程">BIOS中的启动流程</span></h2><ol>
<li>BIOS启动</li>
<li>执行MBR分区代码boot.img、core.img</li>
<li>设置”prefix root cmdpath”环境变量</li>
<li>加载”normal.mod”模块</li>
<li>执行”normal $prefix&#x2F;grub.cfg”命令</li>
<li>根据菜单内容引导内核启动</li>
</ol>
<h2><span id="uefi中的启动流程">UEFI中的启动流程</span></h2><ol>
<li><p>UEFI启动</p>
</li>
<li><p>执行ESP(EFI System Partition)分区中的UEFI应用（BOOTX64.EFI）</p>
<blockquote>
<p>个专门用以安装UEFI应用的分区,ESP分区只能是原生的物理硬盘，<em><strong>不能是LVM或者是RAID</strong></em>，这算是UEFI的缺点。ESP的文件系统格式支持vfat（一般都用FAT32）。</p>
</blockquote>
</li>
<li><p>设置”prefix root cmdpath”环境变量</p>
</li>
<li><p>加载”normal.mod”模块</p>
</li>
<li><p>执行”normal $prefix&#x2F;grub.cfg”命令</p>
</li>
<li><p>根据菜单内容引导内核启动</p>
</li>
</ol>
]]></content>
      <categories>
        <category>BIOS</category>
      </categories>
      <tags>
        <tag>BIOS/UEFI</tag>
      </tags>
  </entry>
  <entry>
    <title>mkisofs 制作grub2可引导的ISO文件</title>
    <url>/2022/05/09/mkisofs-%E5%88%B6%E4%BD%9Cgrub2%E5%8F%AF%E5%BC%95%E5%AF%BC%E7%9A%84ISO%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<h2><span id="从头开始制作grub2可引导光盘">从头开始制作GRUB2可引导光盘</span></h2><ol>
<li><p>一般情况下GRUB2的模块文件位于&#x2F;usr&#x2F;lib&#x2F;grub&#x2F;i386-pc目录下，下面先拷贝模块</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ mkdir -pv /tmp/iso/boot/grub/</span><br><span class="line">$ cp /usr/lib/grub/i386-pc/ /tmp/iso/boot/grub/ -R</span><br><span class="line"></span><br></pre></td></tr></table></figure></blockquote>
</li>
</ol>
<span id="more"></span>

<ol start="2">
<li><p>接着生成GRUB2的内核</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cd /tmp/iso/boot/grub/i386-pc/</span><br><span class="line">$ grub-mkimage -o core.img biosdisk iso9660 ext2 normal boot configfile linux minicmd -O i386-pc -p /boot/grub</span><br><span class="line">$ cat cdboot.img core.img &gt; g2ldr</span><br><span class="line"></span><br></pre></td></tr></table></figure></blockquote>
</li>
<li><p>生成可引导的ISO文件</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cd /tmp/iso</span><br><span class="line">$ mkisofs -R -J -no-emul-boot -boot-info-table -boot-load-size 4 -b boot/grub/i386-pc/g2ldr -o ../grub.iso .</span><br><span class="line"></span><br></pre></td></tr></table></figure></blockquote>
</li>
<li><p>grub.cfg</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set default=&quot;0&quot;</span><br><span class="line">set timeout=&quot;3&quot;</span><br><span class="line">search --no-floppy --label --set=root  TENGLING</span><br><span class="line">menuentry &quot; x86 &quot;&#123;</span><br><span class="line">linux /boot/x86.bzImage rootfstype=ext4 rootwait console=tty0 console=ttyS0,115200n8 install</span><br><span class="line">init=/sbin/init</span><br><span class="line">initrd /boot/x86.cpio.gz</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></blockquote>
</li>
</ol>
]]></content>
      <categories>
        <category>BIOS</category>
      </categories>
      <tags>
        <tag>grub2</tag>
      </tags>
  </entry>
  <entry>
    <title>RAID1源码分析~同步</title>
    <url>/2022/05/15/RAID1%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E5%90%8C%E6%AD%A5/</url>
    <content><![CDATA[<h1><span id="源码分析">源码分析</span></h1><p>基于4.19.220分析</p>
<h2><span id="raid1_sync_request">raid1_sync_request</span></h2><p>该函数由MD模块发起</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (!mempool_initialized(&amp;conf-&gt;r1buf_pool))</span><br><span class="line">    if (init_resync(conf))</span><br><span class="line">	    return 0;</span><br></pre></td></tr></table></figure>
<p>如果没有申请同步缓存，则申请缓存。</p>
<span id="more"></span>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">max_sector = mddev-&gt;dev_sectors;</span><br><span class="line">if (sector_nr &gt;= max_sector) &#123;</span><br><span class="line">	/* If we aborted, we need to abort the</span><br><span class="line">	 * sync on the &#x27;current&#x27; bitmap chunk (there will</span><br><span class="line">	 * only be one in raid1 resync.</span><br><span class="line">	 * We can find the current addess in mddev-&gt;curr_resync</span><br><span class="line">	 */</span><br><span class="line">	if (mddev-&gt;curr_resync &lt; max_sector) /* aborted */</span><br><span class="line">		md_bitmap_end_sync(mddev-&gt;bitmap, mddev-&gt;curr_resync,</span><br><span class="line">				   &amp;sync_blocks, 1);</span><br><span class="line">	else /* completed sync */</span><br><span class="line">		conf-&gt;fullsync = 0;</span><br><span class="line">	md_bitmap_close_sync(mddev-&gt;bitmap);</span><br><span class="line">	close_sync(conf);</span><br><span class="line">	if (mddev_is_clustered(mddev)) &#123;</span><br><span class="line">		conf-&gt;cluster_sync_low = 0;</span><br><span class="line">		conf-&gt;cluster_sync_high = 0;</span><br><span class="line">	&#125;</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同步位置如果不在阵列中，更新bitmap并返回。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (mddev-&gt;bitmap == NULL &amp;&amp;</span><br><span class="line">    mddev-&gt;recovery_cp == MaxSector &amp;&amp;</span><br><span class="line">    !test_bit(MD_RECOVERY_REQUESTED, &amp;mddev-&gt;recovery) &amp;&amp;</span><br><span class="line">    conf-&gt;fullsync == 0) &#123;</span><br><span class="line">	*skipped = 1;</span><br><span class="line">	return max_sector - sector_nr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果bitmap没有设置、阵列没有同步需求（阵列正常工作是设置<code>mddev-&gt;recovery_cp = MaxSector</code>）、整列没有设置同步请求、阵列不需要完全同步则跳过。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* before building a request, check if we can skip these blocks..</span><br><span class="line"> * This call the bitmap_start_sync doesn&#x27;t actually record anything</span><br><span class="line"> */</span><br><span class="line">if (!md_bitmap_start_sync(mddev-&gt;bitmap, sector_nr, &amp;sync_blocks, 1) &amp;&amp;</span><br><span class="line">    !conf-&gt;fullsync &amp;&amp; !test_bit(MD_RECOVERY_REQUESTED, &amp;mddev-&gt;recovery)) &#123;</span><br><span class="line">	/* We can skip this block, and probably several more */</span><br><span class="line">	*skipped = 1;</span><br><span class="line">	return sync_blocks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果bitmap认为无需同步、阵列未设置完全同步、该请求不是用户下发的则跳过。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * If there is non-resync activity waiting for a turn, then let it</span><br><span class="line"> * though before starting on this new sync request.</span><br><span class="line"> */</span><br><span class="line">if (atomic_read(&amp;conf-&gt;nr_waiting[idx]))</span><br><span class="line">	schedule_timeout_uninterruptible(1);</span><br></pre></td></tr></table></figure>
<p>如果阵列忙于处理其他非同步类的操作（<code>raid1_write_request</code>会调用<code>wait_barrier</code>对<code>nr_waiting</code>进行相应的操作），则延时等待一会（<code>raise_barrier</code>中还会对<code>nr_waiting</code>处理）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* we are incrementing sector_nr below. To be safe, we check against</span><br><span class="line">* sector_nr + two times RESYNC_SECTORS</span><br><span class="line">*/</span><br><span class="line"></span><br><span class="line">md_bitmap_cond_end_sync(mddev-&gt;bitmap, sector_nr,</span><br><span class="line">	mddev_is_clustered(mddev) &amp;&amp; (sector_nr + 2 * RESYNC_SECTORS &gt; conf-&gt;cluster_sync_high));</span><br></pre></td></tr></table></figure>
<p><em>待处理</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (raise_barrier(conf, sector_nr))</span><br><span class="line">	return 0;</span><br></pre></td></tr></table></figure>
<p>在阵列上设置barrier，保证盘阵的同步操作不受其他操作打扰，同时检查如果阵列正在进行访问，则等待其完成（该障碍在同步结束后在<code>end_sync_write</code>中调用<code>put_buf</code>清除）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">r1_bio = raid1_alloc_init_r1buf(conf);</span><br><span class="line">rcu_read_lock();</span><br><span class="line">/*</span><br><span class="line"> * If we get a correctably read error during resync or recovery,</span><br><span class="line"> * we might want to read from a different device.  So we</span><br><span class="line"> * flag all drives that could conceivably be read from for READ,</span><br><span class="line"> * and any others (which will be non-In_sync devices) for WRITE.</span><br><span class="line"> * If a read fails, we try reading from something else for which READ</span><br><span class="line"> * is OK.</span><br><span class="line"> */</span><br><span class="line">r1_bio-&gt;mddev = mddev;</span><br><span class="line">r1_bio-&gt;sector = sector_nr;</span><br><span class="line">r1_bio-&gt;state = 0;</span><br><span class="line">set_bit(R1BIO_IsSync, &amp;r1_bio-&gt;state);</span><br></pre></td></tr></table></figure>
<p>申请同步缓存空间<code>r1_bio</code>并设置<code>R1BIO_IsSync</code>同步状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">good_sectors = align_to_barrier_unit_end(sector_nr, good_sectors);</span><br></pre></td></tr></table></figure>
<p>计算可同步的最大sectors数</p>
<blockquote>
<p>屏障保护数据最大64M，<code>raid1_make_request</code>中也会基于次函数计算可下发的sectors</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for (i = 0; i &lt; conf-&gt;raid_disks * 2; i++) &#123;</span><br><span class="line">	struct md_rdev *rdev;</span><br><span class="line">	bio = r1_bio-&gt;bios[i];</span><br></pre></td></tr></table></figure>
<p>遍历阵列中所有的盘，进入循环</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rdev = rcu_dereference(conf-&gt;mirrors[i].rdev);</span><br><span class="line">if (rdev == NULL ||</span><br><span class="line">    test_bit(Faulty, &amp;rdev-&gt;flags)) &#123;</span><br><span class="line">	if (i &lt; conf-&gt;raid_disks)</span><br><span class="line">		still_degraded = 1;</span><br><span class="line">&#125; else if (!test_bit(In_sync, &amp;rdev-&gt;flags)) &#123;</span><br><span class="line">	bio_set_op_attrs(bio, REQ_OP_WRITE, 0);</span><br><span class="line">	bio-&gt;bi_end_io = end_sync_write;</span><br><span class="line">	write_targets ++;</span><br><span class="line">&#125; else &#123;</span><br></pre></td></tr></table></figure>
<ol>
<li>如果盘不存在或是<code>Faulty</code>盘，则设置<code>still_degraded = 1</code>，继续遍历下一个盘</li>
<li>如果盘没有设置<code>In_sync</code>标记，则设置为写盘<br>&lt; 阵列中除热备盘及坏掉盘都会设置<code>In_sync</code>标记为active盘，此处设置为阵列添加新盘或热备盘替换新盘</li>
<li>其余盘设置为读盘</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* may need to read from here */</span><br><span class="line">sector_t first_bad = MaxSector;</span><br><span class="line">int bad_sectors;</span><br><span class="line"></span><br><span class="line">if (is_badblock(rdev, sector_nr, good_sectors,</span><br><span class="line">		&amp;first_bad, &amp;bad_sectors)) &#123;</span><br><span class="line">	if (first_bad &gt; sector_nr)</span><br><span class="line">		good_sectors = first_bad - sector_nr;</span><br><span class="line">	else &#123;</span><br><span class="line">		bad_sectors -= (sector_nr - first_bad);</span><br><span class="line">		if (min_bad == 0 ||</span><br><span class="line">		    min_bad &gt; bad_sectors)</span><br><span class="line">			min_bad = bad_sectors;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查看该盘是否存在坏块，如果有坏块且坏块起始位置大于同步的位置，则计算出坏块之前正常的位置进行同步；如果坏块出现位置在同步位置之前，则重新计算坏块大小（<code>bad_sectors -= (sector_nr - first_bad)</code>）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (sector_nr &lt; first_bad) &#123;</span><br><span class="line">	if (test_bit(WriteMostly, &amp;rdev-&gt;flags)) &#123;</span><br><span class="line">		if (wonly &lt; 0)</span><br><span class="line">			wonly = i;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		if (disk &lt; 0)</span><br><span class="line">			disk = i;</span><br><span class="line">	&#125;</span><br><span class="line">	bio_set_op_attrs(bio, REQ_OP_READ, 0);</span><br><span class="line">	bio-&gt;bi_end_io = end_sync_read;</span><br><span class="line">	read_targets++;</span><br><span class="line">&#125; else if (!test_bit(WriteErrorSeen, &amp;rdev-&gt;flags) &amp;&amp;</span><br><span class="line">	test_bit(MD_RECOVERY_SYNC, &amp;mddev-&gt;recovery) &amp;&amp;</span><br><span class="line">	!test_bit(MD_RECOVERY_CHECK, &amp;mddev-&gt;recovery)) &#123;</span><br><span class="line">	/*</span><br><span class="line">	 * The device is suitable for reading (InSync),</span><br><span class="line">	 * but has bad block(s) here. Let&#x27;s try to correct them,</span><br><span class="line">	 * if we are doing resync or repair. Otherwise, leave</span><br><span class="line">	 * this device alone for this sync request.</span><br><span class="line">	 */</span><br><span class="line">	bio_set_op_attrs(bio, REQ_OP_WRITE, 0);</span><br><span class="line">	bio-&gt;bi_end_io = end_sync_write;</span><br><span class="line">	write_targets++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>没有坏块（<code>first_bad = MaxSector</code>）或坏块位置大于同步起始位置（同步的<code>good_sectors</code>已被重新计算），设置盘为读盘，并初始化相应<code>bio</code>结构<blockquote>
<ul>
<li>如果此盘被设置成<code>WriteMostly</code>则设置该盘为主要读盘（系统发起的同步，只有一块读盘，其他为写盘，设置盘设置为该标记，则设置其为读盘）</li>
<li>如果没有设置读盘<code>disk &lt; 0</code>则设置其为读盘</li>
</ul>
</blockquote>
</li>
<li>存在坏块且同步位置处于坏块之中，该盘没有出现过写错误、阵列有同步请求、不是检查性的同步，设置该盘为写盘（对盘进行写操作，尝试恢复坏块）</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (rdev &amp;&amp; bio-&gt;bi_end_io) &#123;</span><br><span class="line">	atomic_inc(&amp;rdev-&gt;nr_pending);</span><br><span class="line">	bio-&gt;bi_iter.bi_sector = sector_nr + rdev-&gt;data_offset;</span><br><span class="line">	bio_set_dev(bio, rdev-&gt;bdev);</span><br><span class="line">	if (test_bit(FailFast, &amp;rdev-&gt;flags))</span><br><span class="line">		bio-&gt;bi_opf |= MD_FAILFAST;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>磁盘存在并且设置读写盘，<code>nr_pending</code>加一标记正在处理请求数，更新<code>bio</code>结构</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rcu_read_unlock();</span><br><span class="line">if (disk &lt; 0)</span><br><span class="line">	disk = wonly;</span><br><span class="line">r1_bio-&gt;read_disk = disk;</span><br></pre></td></tr></table></figure>
<p>结束遍历阵列后，判断如果没有设置读盘，则选取<code>WriteMostly</code>为读盘</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (read_targets == 0 &amp;&amp; min_bad &gt; 0) &#123;</span><br><span class="line">	/* These sectors are bad on all InSync devices, so we</span><br><span class="line">	 * need to mark them bad on all write targets</span><br><span class="line">	 */</span><br><span class="line">	int ok = 1;</span><br><span class="line">	for (i = 0 ; i &lt; conf-&gt;raid_disks * 2 ; i++)</span><br><span class="line">		if (r1_bio-&gt;bios[i]-&gt;bi_end_io == end_sync_write) &#123;</span><br><span class="line">			struct md_rdev *rdev = conf-&gt;mirrors[i].rdev;</span><br><span class="line">			ok = rdev_set_badblocks(rdev, sector_nr,</span><br><span class="line">						min_bad, 0</span><br><span class="line">				) &amp;&amp; ok;</span><br><span class="line">		&#125;</span><br><span class="line">	set_bit(MD_SB_CHANGE_DEVS, &amp;mddev-&gt;sb_flags);</span><br><span class="line">	*skipped = 1;</span><br><span class="line">	put_buf(r1_bio);</span><br><span class="line">	if (!ok) &#123;</span><br><span class="line">		/* Cannot record the badblocks, so need to</span><br><span class="line">		 * abort the resync.</span><br><span class="line">		 * If there are multiple read targets, could just</span><br><span class="line">		 * fail the really bad ones ???</span><br><span class="line">		 */</span><br><span class="line">		conf-&gt;recovery_disabled = mddev-&gt;recovery_disabled;</span><br><span class="line">		set_bit(MD_RECOVERY_INTR, &amp;mddev-&gt;recovery);</span><br><span class="line">		return 0;</span><br><span class="line">	&#125; else</span><br><span class="line">		return min_bad;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>存在坏块且没有可读盘的时候，把这些坏块标记到可写的盘上，同时更新超级块信息，释放同步缓存，如果标记坏块失败则中断同步返回。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (min_bad &gt; 0 &amp;&amp; min_bad &lt; good_sectors) &#123;</span><br><span class="line">	/* only resync enough to reach the next bad-&gt;good</span><br><span class="line">	 * transition */</span><br><span class="line">	good_sectors = min_bad;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果出现坏块，只同步出现坏块的部分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (test_bit(MD_RECOVERY_SYNC, &amp;mddev-&gt;recovery) &amp;&amp; read_targets &gt; 0)</span><br><span class="line">	/* extra read targets are also write targets */</span><br><span class="line">	write_targets += read_targets-1;</span><br></pre></td></tr></table></figure>
<p>遵循一读多写原则，在标记的写盘中，除去一个主要的读盘，其它也是写盘</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (write_targets == 0 || read_targets == 0) &#123;</span><br><span class="line">	/* There is nowhere to write, so all non-sync</span><br><span class="line">	 * drives must be failed - so we are finished</span><br><span class="line">	 */</span><br><span class="line">	sector_t rv;</span><br><span class="line">	if (min_bad &gt; 0)</span><br><span class="line">		max_sector = sector_nr + min_bad;</span><br><span class="line">	rv = max_sector - sector_nr;</span><br><span class="line">	*skipped = 1;</span><br><span class="line">	put_buf(r1_bio);</span><br><span class="line">	return rv;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果没有读盘或是写盘，则跳过该同步区域</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (max_sector &gt; mddev-&gt;resync_max)</span><br><span class="line">	max_sector = mddev-&gt;resync_max; /* Don&#x27;t do IO beyond here */</span><br><span class="line">if (max_sector &gt; sector_nr + good_sectors)</span><br><span class="line">	max_sector = sector_nr + good_sectors;</span><br><span class="line">nr_sectors = 0;</span><br><span class="line">sync_blocks = 0;</span><br></pre></td></tr></table></figure>
<p>设置最大可同步的sectors</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">do &#123;</span><br><span class="line">	struct page *page;</span><br><span class="line">	int len = PAGE_SIZE;</span><br><span class="line">	if (sector_nr + (len&gt;&gt;9) &gt; max_sector)</span><br><span class="line">		len = (max_sector - sector_nr) &lt;&lt; 9;</span><br><span class="line">	if (len == 0)</span><br><span class="line">		break;</span><br><span class="line">	if (sync_blocks == 0) &#123;</span><br><span class="line">		if (!md_bitmap_start_sync(mddev-&gt;bitmap, sector_nr,</span><br><span class="line">					  &amp;sync_blocks, still_degraded) &amp;&amp;</span><br><span class="line">		    !conf-&gt;fullsync &amp;&amp;</span><br><span class="line">		    !test_bit(MD_RECOVERY_REQUESTED, &amp;mddev-&gt;recovery))</span><br><span class="line">			break;</span><br><span class="line">		if ((len &gt;&gt; 9) &gt; sync_blocks)</span><br><span class="line">			len = sync_blocks&lt;&lt;9;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	for (i = 0 ; i &lt; conf-&gt;raid_disks * 2; i++) &#123;</span><br><span class="line">		struct resync_pages *rp;</span><br><span class="line"></span><br><span class="line">		bio = r1_bio-&gt;bios[i];</span><br><span class="line">		rp = get_resync_pages(bio);</span><br><span class="line">		if (bio-&gt;bi_end_io) &#123;</span><br><span class="line">			page = resync_fetch_page(rp, page_idx);</span><br><span class="line"></span><br><span class="line">			/*</span><br><span class="line">			 * won&#x27;t fail because the vec table is big</span><br><span class="line">			 * enough to hold all these pages</span><br><span class="line">			 */</span><br><span class="line">			bio_add_page(bio, page, len, 0);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	nr_sectors += len&gt;&gt;9;</span><br><span class="line">	sector_nr += len&gt;&gt;9;</span><br><span class="line">	sync_blocks -= (len&gt;&gt;9);</span><br><span class="line">	&#125; while (++page_idx &lt; RESYNC_PAGES);</span><br></pre></td></tr></table></figure>
<p>构建<code>r1_bio</code>中每个<code>bio</code>的<code>vec</code>，将缓存页添加到数组中，直到数组满（<code>while (++page_idx &lt; RESYNC_PAGES)</code>）或是达到数据同步的长度（<code>if (len == 0)</code>）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">r1_bio-&gt;sectors = nr_sectors;</span><br></pre></td></tr></table></figure>
<p>设置需要同步的数据块</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* For a user-requested sync, we read all readable devices and do a</span><br><span class="line"> * compare</span><br><span class="line"> */</span><br><span class="line">if (test_bit(MD_RECOVERY_REQUESTED, &amp;mddev-&gt;recovery)) &#123;</span><br><span class="line">	atomic_set(&amp;r1_bio-&gt;remaining, read_targets);</span><br><span class="line">	for (i = 0; i &lt; conf-&gt;raid_disks * 2 &amp;&amp; read_targets; i++) &#123;</span><br><span class="line">		bio = r1_bio-&gt;bios[i];</span><br><span class="line">		if (bio-&gt;bi_end_io == end_sync_read) &#123;</span><br><span class="line">			read_targets--;</span><br><span class="line">			md_sync_acct_bio(bio, nr_sectors);</span><br><span class="line">			if (read_targets == 1)</span><br><span class="line">				bio-&gt;bi_opf &amp;= ~MD_FAILFAST;</span><br><span class="line">			generic_make_request(bio);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">	atomic_set(&amp;r1_bio-&gt;remaining, 1);</span><br><span class="line">	bio = r1_bio-&gt;bios[r1_bio-&gt;read_disk];</span><br><span class="line">	md_sync_acct_bio(bio, nr_sectors);</span><br><span class="line">	if (read_targets == 1)</span><br><span class="line">		bio-&gt;bi_opf &amp;= ~MD_FAILFAST;</span><br><span class="line">	generic_make_request(bio);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>对于用户发起的同步请求，设置<code>r1_bio-&gt;remaining</code>为<code>read_targets</code>，并对所有的读盘发起读请求</li>
<li>对于系统发起的同步请求，设置<code>r1_bio-&gt;remaining</code>为1，只对一块可读盘（<code>r1_bio-&gt;read_disk</code>设置的盘）发起读请求<blockquote>
<p><code>r1_bio-&gt;remaining</code>在<code>end_sync_read</code>中标记是否完成所有的读操作</p>
</blockquote>
</li>
</ol>
<h2><span id="end_sync_read">end_sync_read</span></h2><p>同步下发的读操作完成之后调用该函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static void end_sync_read(struct bio *bio)</span><br><span class="line">&#123;</span><br><span class="line">    struct r1bio *r1_bio = get_resync_r1bio(bio);</span><br><span class="line"></span><br><span class="line">    update_head_pos(r1_bio-&gt;read_disk, r1_bio);</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * we have read a block, now it needs to be re-written,</span><br><span class="line">	 * or re-read if the read failed.</span><br><span class="line">	 * We don&#x27;t do much here, just schedule handling by raid1d</span><br><span class="line">	 */</span><br><span class="line">	if (!bio-&gt;bi_status)</span><br><span class="line">		set_bit(R1BIO_Uptodate, &amp;r1_bio-&gt;state);</span><br><span class="line"></span><br><span class="line">	if (atomic_dec_and_test(&amp;r1_bio-&gt;remaining))</span><br><span class="line">		reschedule_retry(r1_bio);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>更新读操作完成的位置</li>
<li>更新<code>R1BIO_Uptodate</code>标记</li>
<li>如果所有读操作完成，则将<code>r1_bio</code>挂到<code>retry_list</code>上唤醒RAID1守护线程</li>
</ol>
<h2><span id="sync_request_write">sync_request_write</span></h2><p>守护线程判读<code>retry_list</code>有需要处理的<code>r1_bio</code>,则调用<code>sync_request_write</code>进行同步写的过程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (!test_bit(R1BIO_Uptodate, &amp;r1_bio-&gt;state))</span><br><span class="line">	/* ouch - failed to read all of that. */</span><br><span class="line">	if (!fix_sync_read_error(r1_bio))</span><br><span class="line">		return;</span><br></pre></td></tr></table></figure>
<p>所有读都失败，尝试对盘进行恢复</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (test_bit(MD_RECOVERY_REQUESTED, &amp;mddev-&gt;recovery))</span><br><span class="line">	process_checks(r1_bio);</span><br></pre></td></tr></table></figure>
<p>对于用户下发的同步请求，首先找到第一个读完成的盘，以此作为基准和其他读盘的内容比较，如果不相等则将内容拷贝过去，如果相等则设置<code>sbio-&gt;bi_end_io = NULL</code>不对这个盘进行写操作</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * schedule writes</span><br><span class="line"> */</span><br><span class="line">atomic_set(&amp;r1_bio-&gt;remaining, 1);</span><br><span class="line">for (i = 0; i &lt; disks ; i++) &#123;</span><br><span class="line">	wbio = r1_bio-&gt;bios[i];</span><br><span class="line">	if (wbio-&gt;bi_end_io == NULL ||</span><br><span class="line">	    (wbio-&gt;bi_end_io == end_sync_read &amp;&amp;</span><br><span class="line">	     (i == r1_bio-&gt;read_disk ||</span><br><span class="line">	      !test_bit(MD_RECOVERY_SYNC, &amp;mddev-&gt;recovery))))</span><br><span class="line">		continue;</span><br><span class="line">	if (test_bit(Faulty, &amp;conf-&gt;mirrors[i].rdev-&gt;flags)) &#123;</span><br><span class="line">		abort_sync_write(mddev, r1_bio);</span><br><span class="line">		continue;</span><br><span class="line">	&#125;</span><br><span class="line">	bio_set_op_attrs(wbio, REQ_OP_WRITE, 0);</span><br><span class="line">	if (test_bit(FailFast, &amp;conf-&gt;mirrors[i].rdev-&gt;flags))</span><br><span class="line">		wbio-&gt;bi_opf |= MD_FAILFAST;</span><br><span class="line">	wbio-&gt;bi_end_io = end_sync_write;</span><br><span class="line">	atomic_inc(&amp;r1_bio-&gt;remaining);</span><br><span class="line">	md_sync_acct(conf-&gt;mirrors[i].rdev-&gt;bdev, bio_sectors(wbio));</span><br><span class="line">	generic_make_request(wbio);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>遍历所有盘的<code>bio</code></p>
<ol>
<li>如果<code>bi_end_io</code>为空（用户态下发同步请求中，不需要修改的盘）、IO类型为读并且设置为读盘或是没有同步请求跳过</li>
<li>如果盘类型是<code>Faulty</code>跳过</li>
<li>设置io类型为写、增加IO计数</li>
<li>对同步IO进行提交</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (atomic_dec_and_test(&amp;r1_bio-&gt;remaining)) &#123;</span><br><span class="line">	/* if we&#x27;re here, all write(s) have completed, so clean up */</span><br><span class="line">	int s = r1_bio-&gt;sectors;</span><br><span class="line">	if (test_bit(R1BIO_MadeGood, &amp;r1_bio-&gt;state) ||</span><br><span class="line">	    test_bit(R1BIO_WriteError, &amp;r1_bio-&gt;state))</span><br><span class="line">		reschedule_retry(r1_bio);</span><br><span class="line">	else &#123;</span><br><span class="line">		put_buf(r1_bio);</span><br><span class="line">		md_done_sync(mddev, s, 1);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所有读请求完成，释放资源</p>
<h2><span id="end_sync_write">end_sync_write</span></h2><p>同步写完成之后，执行<code>end_sync_write</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static void end_sync_write(struct bio *bio)</span><br><span class="line">&#123;</span><br><span class="line">	int uptodate = !bio-&gt;bi_status;</span><br><span class="line">	struct r1bio *r1_bio = get_resync_r1bio(bio);</span><br><span class="line">	struct mddev *mddev = r1_bio-&gt;mddev;</span><br><span class="line">	struct r1conf *conf = mddev-&gt;private;</span><br><span class="line">	sector_t first_bad;</span><br><span class="line">	int bad_sectors;</span><br><span class="line">	struct md_rdev *rdev = conf-&gt;mirrors[find_bio_disk(r1_bio, bio)].rdev;</span><br><span class="line"></span><br><span class="line">	if (!uptodate) &#123;</span><br><span class="line">		abort_sync_write(mddev, r1_bio);</span><br><span class="line">		set_bit(WriteErrorSeen, &amp;rdev-&gt;flags);</span><br><span class="line">		if (!test_and_set_bit(WantReplacement, &amp;rdev-&gt;flags))</span><br><span class="line">			set_bit(MD_RECOVERY_NEEDED, &amp;</span><br><span class="line">				mddev-&gt;recovery);</span><br><span class="line">		set_bit(R1BIO_WriteError, &amp;r1_bio-&gt;state);</span><br><span class="line">	&#125; else if (is_badblock(rdev, r1_bio-&gt;sector, r1_bio-&gt;sectors,</span><br><span class="line">			       &amp;first_bad, &amp;bad_sectors) &amp;&amp;</span><br><span class="line">		   !is_badblock(conf-&gt;mirrors[r1_bio-&gt;read_disk].rdev,</span><br><span class="line">				r1_bio-&gt;sector,</span><br><span class="line">				r1_bio-&gt;sectors,</span><br><span class="line">				&amp;first_bad, &amp;bad_sectors)</span><br><span class="line">		)</span><br><span class="line">		set_bit(R1BIO_MadeGood, &amp;r1_bio-&gt;state);</span><br><span class="line"></span><br><span class="line">	if (atomic_dec_and_test(&amp;r1_bio-&gt;remaining)) &#123;</span><br><span class="line">		int s = r1_bio-&gt;sectors;</span><br><span class="line">		if (test_bit(R1BIO_MadeGood, &amp;r1_bio-&gt;state) ||</span><br><span class="line">		    test_bit(R1BIO_WriteError, &amp;r1_bio-&gt;state))</span><br><span class="line">			reschedule_retry(r1_bio);</span><br><span class="line">		else &#123;</span><br><span class="line">			put_buf(r1_bio);</span><br><span class="line">			md_done_sync(mddev, s, uptodate);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>写失败调用<code>abort_sync_write</code>设置bitmap相应位为需要进行同步</li>
<li>如果写盘同步区域存在坏块并且读盘的位置没有坏块则设置标记<code>R1BIO_MadeGood</code>（<code>r1_bio</code>需要重新挂到<code>retry_list</code>上重新进行调度）</li>
<li>如果所有写盘的请求全部完成且没有错误则清除阵列障碍释放缓存否则重新唤醒守护进程处理</li>
</ol>
<h1><span id="总体流程">总体流程</span></h1><p>此流程<a href="https://www.cnblogs.com/fangpei/p/4676891.html">fangpei</a> 总结很好，因此搬用到这，具体连接见参考<br>同步的大流程是先读，后写。所以是分两个阶段，<code>sync_request</code>完成第一个阶段，<code>sync_request_write</code>完成第二个阶段。第一个阶段由MD发起(md_do_sync)，第二个阶段由守护进程发起。</p>
<h2><span id="用户下发同步请求">用户下发同步请求</span></h2><p>该请求下发到raid1层，首先进入同步读函数<code>sync_request</code>。在正常的成员盘中，将所有active可用的盘（<code>rdev-&gt;flags</code>中有<code>In_sync</code>标记）设置为read盘，而所有不可用的不做设置。对每一个可用盘对应的<code>bios[]</code>都单独申请页结构，对所有的read盘下发读请求，读成功之后，每块read盘对应的<code>bios[]</code>的页结构中都存放有该read盘中的内容。当所有读请求都完成之时，将<code>r1_bio</code>添加到<code>retry_list</code>队列，交由守护进程处理。接着，切换到守护进程，守护进程获取到<code>retry_list</code>中的内容，判断<code>r1_bio-&gt;state为R1BIO_IsSync</code>。然后调用同步写函数<code>sync_request_write</code>来进行同步写操作。接着顺序遍历成员盘的<code>bios[]</code>，找到第一个读成功的bio。然后将其与其他所有读成功的bio做对比，如果有不一致的页，那么将不一致的页拷贝到这些其他的读成功bio中，并将这些bio也标记 为写请求，并将所有的写请求下发，写成功返回,所有页都是一致的盘，则不用下发写bio。</p>
<h2><span id="系下发的同步请求">系下发的同步请求</span></h2><p>如果是非用户发起的同步请求。该请求下发到raid1层，首先进入同步读函数<code>sync_request</code>。在正常的成员盘中，将有<code>In_sync</code>标记的盘（标记在<code>rdev-&gt;flags</code>上）设置为read盘，将没有<code>In_sync</code>标记的盘设置为write盘，对所有找不到的或<code>Faulty</code>盘不设置读写。所有成员盘对应的<code>bios[]</code>都共享一份页结构，选取一个read盘下发读请求，读成功之后，该块read盘中的内容读取到<code>bios[]</code>的页结构中。当读请求完成之时，将<code>r1_bio</code>添加到<code>retry_list</code>队列，交由守护进程处理。接着，切换到守护进程，守护进程获取到<code>retry_list</code>中的内容，判断<code>r1_bio-&gt;state</code>为<code>R1BIO_IsSync</code>。然后调用同步写函数<code>sync_request_write</code>来进行同步写操作。接着该函数将除了执行读操作的盘之外的所有盘都标记为WRITE盘，并将写操作下发。由于所有成员盘的bio都共享一份页结构，所以写操作下发的均为读操作读取到这份页 结构的所有内容。</p>
<h2><span id="同步的三种情况">同步的三种情况</span></h2><p>其实raid1的sync&#x2F;recovery流程对于三种不同的情景有不同的处理方法，整体总结如下： </p>
<ol>
<li>用户发起的sync&#x2F;recovery，有<code>REQUESTED</code>标志位，将有<code>In_sync</code>标记的盘的内容都读到内存，然后选出内存中一个盘的内容作为“标杆”，用其他盘的内容与其对比。如果有不一致的情况，则将“标杆”盘的内容拷贝其他盘内容的内存区域，并写回其他盘；如果是一致的情况，则不需要拷贝，也不需要写回。 </li>
<li>系统发起的recovery，这种情况是有盘坏掉，移除旧盘添加新盘之后的recovery。没有<code>REQUESTED</code>标志位，选出一个<code>In_sync</code>标志位的盘内容读到内存，并写到新盘中。（新盘没有In_sync标志位） </li>
<li>系统掉电重启时的sync&#x2F;recovery，没有<code>REQUESTED</code>标志位，选出一个有<code>In_sync</code>标志位的盘，并将其内容读到内存，然后写到其他所有盘中。</li>
</ol>
<h3><span id="区别">区别</span></h3><ol>
<li>读出所有盘的数据，并进行对比，再决定是否拷贝和写回； </li>
<li>读出一个正常盘的数据，写入到新盘中。其他正常盘均没有读写操作。 </li>
<li>读出一个正常盘的数据，写入到其他所有盘。</li>
</ol>
<h1><span id="参考">参考</span></h1><ul>
<li><a href="https://www.cnblogs.com/fangpei/p/4676891.html">https://www.cnblogs.com/fangpei/p/4676891.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>MD</tag>
      </tags>
  </entry>
  <entry>
    <title>RAID1源码分析~写.md</title>
    <url>/2022/05/21/RAID1%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E5%86%99/</url>
    <content><![CDATA[<h1><span id="源码分析">源码分析</span></h1><p>基于4.19.22.0</p>
<h2><span id="raid1_write_request">raid1_write_request</span></h2><p>该函数为请求处理，由MD模块<code>md_make_request</code>下发请求调用<code>raid1_make_request</code>处理，在处理写请求之前还需要调用<code>md_write_start</code>来等待超级块更新，更新完之后调用<code>raid1_write_request</code> 处理写请求</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (mddev_is_clustered(mddev) &amp;&amp;</span><br><span class="line">     md_cluster_ops-&gt;area_resyncing(mddev, WRITE,</span><br><span class="line">	     bio-&gt;bi_iter.bi_sector, bio_end_sector(bio))) &#123;</span><br><span class="line"></span><br><span class="line">	DEFINE_WAIT(w);</span><br><span class="line">	for (;;) &#123;</span><br><span class="line">		prepare_to_wait(&amp;conf-&gt;wait_barrier,</span><br><span class="line">				&amp;w, TASK_IDLE);</span><br><span class="line">		if (!md_cluster_ops-&gt;area_resyncing(mddev, WRITE,</span><br><span class="line">						bio-&gt;bi_iter.bi_sector,</span><br><span class="line">						bio_end_sector(bio)))</span><br><span class="line">			break;</span><br><span class="line">		schedule();</span><br><span class="line">	&#125;</span><br><span class="line">	finish_wait(&amp;conf-&gt;wait_barrier, &amp;w);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><em>待分析</em></p>
<span id="more"></span>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Register the new request and wait if the reconstruction</span><br><span class="line"> * thread has put up a bar for new requests.</span><br><span class="line"> * Continue immediately if no resync is active currently.</span><br><span class="line"> */</span><br><span class="line">wait_barrier(conf, bio-&gt;bi_iter.bi_sector);</span><br></pre></td></tr></table></figure>
<p>等待阵列同步过程中构造的屏障解除，才能下发写操作</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">r1_bio = alloc_r1bio(mddev, bio);</span><br><span class="line">r1_bio-&gt;sectors = max_write_sectors;</span><br></pre></td></tr></table></figure>
<p>构造<code>r1_bio</code>结构</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (conf-&gt;pending_count &gt;= max_queued_requests) &#123;</span><br><span class="line">	md_wakeup_thread(mddev-&gt;thread);</span><br><span class="line">	raid1_log(mddev, &quot;wait queued&quot;);</span><br><span class="line">	wait_event(conf-&gt;wait_barrier,</span><br><span class="line">		   conf-&gt;pending_count &lt; max_queued_requests);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下发的写操作大于<code>max_queued_requests</code>,则等待已提交写操作完成知道符合<code>conf-&gt;pending_count &lt; max_queued_requests</code>条件</p>
<blockquote>
<p><code>conf-&gt;pending_count</code>在bio加到<code>pending_bio_list</code>时递增，在<code>flush_pending_writes</code>中刷写请求到磁盘是置零</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">disks = conf-&gt;raid_disks * 2;</span><br><span class="line">retry_write:</span><br><span class="line">blocked_rdev = NULL;</span><br><span class="line">rcu_read_lock();</span><br><span class="line">max_sectors = r1_bio-&gt;sectors;</span><br><span class="line">for (i = 0;  i &lt; disks; i++) &#123;</span><br><span class="line">	struct md_rdev *rdev = rcu_dereference(conf-&gt;mirrors[i].rdev);</span><br><span class="line">	if (rdev &amp;&amp; unlikely(test_bit(Blocked, &amp;rdev-&gt;flags))) &#123;</span><br><span class="line">		atomic_inc(&amp;rdev-&gt;nr_pending);</span><br><span class="line">		blocked_rdev = rdev;</span><br><span class="line">		break;</span><br><span class="line">	&#125;</span><br><span class="line">	r1_bio-&gt;bios[i] = NULL;</span><br><span class="line">	if (!rdev || test_bit(Faulty, &amp;rdev-&gt;flags)) &#123;</span><br><span class="line">		if (i &lt; conf-&gt;raid_disks)</span><br><span class="line">			set_bit(R1BIO_Degraded, &amp;r1_bio-&gt;state);</span><br><span class="line">		continue;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	atomic_inc(&amp;rdev-&gt;nr_pending);</span><br><span class="line">	if (test_bit(WriteErrorSeen, &amp;rdev-&gt;flags)) &#123;</span><br><span class="line">		sector_t first_bad;</span><br><span class="line">		int bad_sectors;</span><br><span class="line">		int is_bad;</span><br><span class="line"></span><br><span class="line">		is_bad = is_badblock(rdev, r1_bio-&gt;sector, max_sectors,</span><br><span class="line">				     &amp;first_bad, &amp;bad_sectors);</span><br><span class="line">		if (is_bad &lt; 0) &#123;</span><br><span class="line">			/* mustn&#x27;t write here until the bad block is</span><br><span class="line">			 * acknowledged*/</span><br><span class="line">			set_bit(BlockedBadBlocks, &amp;rdev-&gt;flags);</span><br><span class="line">			blocked_rdev = rdev;</span><br><span class="line">			break;</span><br><span class="line">		&#125;</span><br><span class="line">		if (is_bad &amp;&amp; first_bad &lt;= r1_bio-&gt;sector) &#123;</span><br><span class="line">			/* Cannot write here at all */</span><br><span class="line">			bad_sectors -= (r1_bio-&gt;sector - first_bad);</span><br><span class="line">			if (bad_sectors &lt; max_sectors)</span><br><span class="line">				/* mustn&#x27;t write more than bad_sectors</span><br><span class="line">				 * to other devices yet</span><br><span class="line">				 */</span><br><span class="line">				max_sectors = bad_sectors;</span><br><span class="line">			rdev_dec_pending(rdev, mddev);</span><br><span class="line">			/* We don&#x27;t set R1BIO_Degraded as that</span><br><span class="line">			 * only applies if the disk is</span><br><span class="line">			 * missing, so it might be re-added,</span><br><span class="line">			 * and we want to know to recover this</span><br><span class="line">			 * chunk.</span><br><span class="line">			 * In this case the device is here,</span><br><span class="line">			 * and the fact that this chunk is not</span><br><span class="line">			 * in-sync is recorded in the bad</span><br><span class="line">			 * block log</span><br><span class="line">			 */</span><br><span class="line">			continue;</span><br><span class="line">		&#125;</span><br><span class="line">		if (is_bad) &#123;</span><br><span class="line">			int good_sectors = first_bad - r1_bio-&gt;sector;</span><br><span class="line">			if (good_sectors &lt; max_sectors)</span><br><span class="line">				max_sectors = good_sectors;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	r1_bio-&gt;bios[i] = bio;</span><br><span class="line">&#125;</span><br><span class="line">rcu_read_unlock();</span><br></pre></td></tr></table></figure>
<p>循环遍历阵列，处理阻塞盘及坏块</p>
<ol>
<li>如果盘存在并且阻塞了，则跳出循环等待阻塞解除再重新开始循坏</li>
<li>如果盘不存在或者盘坏了，则设置<code>R1BIO_Degraded</code>后，继续遍历下个盘</li>
<li>增加盘下发IO计数<code>nr_pending</code></li>
<li>根据<code>WriteErrorSeen</code>标记进行坏块处理，避免对坏块进行读写<ol>
<li>如果存在未知坏块，则设置<code>BlockedBadBlocks</code>标记，跳出循坏阻塞，等待磁盘修复后阻塞解除再重新开始循环</li>
<li>存在已知坏块且坏块起始位置小于写入位置，则计算出实际可写sectors（其他盘写sectors不能超过这块盘的坏块）</li>
<li>存在已知坏块且坏块起始位置大于写入位置，则计算出实际可写sectors（<code>good_sectors = first_bad - r1_bio-&gt;sector</code>）</li>
</ol>
</li>
<li><code>bios[]</code>指向实际下发的<code>bio</code></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (unlikely(blocked_rdev)) &#123;</span><br><span class="line">	/* Wait for this device to become unblocked */</span><br><span class="line">	int j;</span><br><span class="line"></span><br><span class="line">	for (j = 0; j &lt; i; j++)</span><br><span class="line">		if (r1_bio-&gt;bios[j])</span><br><span class="line">			rdev_dec_pending(conf-&gt;mirrors[j].rdev, mddev);</span><br><span class="line">	r1_bio-&gt;state = 0;</span><br><span class="line">	allow_barrier(conf, bio-&gt;bi_iter.bi_sector);</span><br><span class="line">	raid1_log(mddev, &quot;wait rdev %d blocked&quot;, blocked_rdev-&gt;raid_disk);</span><br><span class="line">	md_wait_for_blocked_rdev(blocked_rdev, mddev);</span><br><span class="line">	wait_barrier(conf, bio-&gt;bi_iter.bi_sector);</span><br><span class="line">	goto retry_write;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果存在阻塞盘，则等待阻塞解除后继续遍历</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (max_sectors &lt; bio_sectors(bio)) &#123;</span><br><span class="line">	struct bio *split = bio_split(bio, max_sectors,</span><br><span class="line">				      GFP_NOIO, &amp;conf-&gt;bio_split);</span><br><span class="line">	bio_chain(split, bio);</span><br><span class="line">	generic_make_request(bio);</span><br><span class="line">	bio = split;</span><br><span class="line">	r1_bio-&gt;master_bio = bio;</span><br><span class="line">	r1_bio-&gt;sectors = max_sectors;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果写请求携带的sectors大于<code>max_sectors</code>，对bio进行切分处理</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">atomic_set(&amp;r1_bio-&gt;remaining, 1);</span><br><span class="line">atomic_set(&amp;r1_bio-&gt;behind_remaining, 0);</span><br></pre></td></tr></table></figure>
<p>未提交请求数<code>remaining</code>、延迟写的未提交的请求数<code>behind_remaining</code></p>
<blockquote>
<p>未提交指未下发到盘的驱动队列中，此值在盘BIO构造好之后增加，BIO完成之后调用<code>r1_bio_write_done</code>减少</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">first_clone = 1;</span><br><span class="line"></span><br><span class="line">for (i = 0; i &lt; disks; i++) &#123;</span><br><span class="line">	struct bio *mbio = NULL;</span><br><span class="line">	if (!r1_bio-&gt;bios[i])</span><br><span class="line">		continue;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	if (first_clone) &#123;</span><br><span class="line">		/* do behind I/O ?</span><br><span class="line">		 * Not if there are too many, or cannot</span><br><span class="line">		 * allocate memory, or a reader on WriteMostly</span><br><span class="line">		 * is waiting for behind writes to flush */</span><br><span class="line">		if (bitmap &amp;&amp;</span><br><span class="line">		    (atomic_read(&amp;bitmap-&gt;behind_writes)</span><br><span class="line">		     &lt; mddev-&gt;bitmap_info.max_write_behind) &amp;&amp;</span><br><span class="line">		    !waitqueue_active(&amp;bitmap-&gt;behind_wait)) &#123;</span><br><span class="line">			alloc_behind_master_bio(r1_bio, bio);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		md_bitmap_startwrite(bitmap, r1_bio-&gt;sector, r1_bio-&gt;sectors,</span><br><span class="line">				     test_bit(R1BIO_BehindIO, &amp;r1_bio-&gt;state));</span><br><span class="line">		first_clone = 0;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (r1_bio-&gt;behind_master_bio)</span><br><span class="line">		mbio = bio_clone_fast(r1_bio-&gt;behind_master_bio,</span><br><span class="line">				      GFP_NOIO, &amp;mddev-&gt;bio_set);</span><br><span class="line">	else</span><br><span class="line">		mbio = bio_clone_fast(bio, GFP_NOIO, &amp;mddev-&gt;bio_set);</span><br><span class="line"></span><br><span class="line">	if (r1_bio-&gt;behind_master_bio) &#123;</span><br><span class="line">		if (test_bit(WriteMostly, &amp;conf-&gt;mirrors[i].rdev-&gt;flags))</span><br><span class="line">			atomic_inc(&amp;r1_bio-&gt;behind_remaining);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	r1_bio-&gt;bios[i] = mbio;</span><br><span class="line"></span><br><span class="line">	mbio-&gt;bi_iter.bi_sector	= (r1_bio-&gt;sector +</span><br><span class="line">			   conf-&gt;mirrors[i].rdev-&gt;data_offset);</span><br><span class="line">	bio_set_dev(mbio, conf-&gt;mirrors[i].rdev-&gt;bdev);</span><br><span class="line">	mbio-&gt;bi_end_io	= raid1_end_write_request;</span><br><span class="line">	mbio-&gt;bi_opf = bio_op(bio) | (bio-&gt;bi_opf &amp; (REQ_SYNC | REQ_FUA));</span><br><span class="line">	if (test_bit(FailFast, &amp;conf-&gt;mirrors[i].rdev-&gt;flags) &amp;&amp;</span><br><span class="line">	    !test_bit(WriteMostly, &amp;conf-&gt;mirrors[i].rdev-&gt;flags) &amp;&amp;</span><br><span class="line">	    conf-&gt;raid_disks - mddev-&gt;degraded &gt; 1)</span><br><span class="line">		mbio-&gt;bi_opf |= MD_FAILFAST;</span><br><span class="line">	mbio-&gt;bi_private = r1_bio;</span><br><span class="line"></span><br><span class="line">	atomic_inc(&amp;r1_bio-&gt;remaining);</span><br><span class="line"></span><br><span class="line">	if (mddev-&gt;gendisk)</span><br><span class="line">		trace_block_bio_remap(mbio-&gt;bi_disk-&gt;queue,</span><br><span class="line">				      mbio, disk_devt(mddev-&gt;gendisk),</span><br><span class="line">				      r1_bio-&gt;sector);</span><br><span class="line">	/* flush_pending_writes() needs access to the rdev so...*/</span><br><span class="line">	mbio-&gt;bi_disk = (void *)conf-&gt;mirrors[i].rdev;</span><br><span class="line"></span><br><span class="line">	cb = blk_check_plugged(raid1_unplug, mddev, sizeof(*plug));</span><br><span class="line">	if (cb)</span><br><span class="line">		plug = container_of(cb, struct raid1_plug_cb, cb);</span><br><span class="line">	else</span><br><span class="line">		plug = NULL;</span><br><span class="line">	if (plug) &#123;</span><br><span class="line">		bio_list_add(&amp;plug-&gt;pending, mbio);</span><br><span class="line">		plug-&gt;pending_cnt++;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		spin_lock_irqsave(&amp;conf-&gt;device_lock, flags);</span><br><span class="line">		bio_list_add(&amp;conf-&gt;pending_bio_list, mbio);</span><br><span class="line">		conf-&gt;pending_count++;</span><br><span class="line">		spin_unlock_irqrestore(&amp;conf-&gt;device_lock, flags);</span><br><span class="line">		md_wakeup_thread(mddev-&gt;thread);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>遍历所有盘的提交BIO</p>
<ol>
<li>克隆BIO，如果未设置behind模式，则从上层BIO克隆到<code>bios[]</code>；如果设置了behind模式，则首先复制一份BIO到<code>behind_master_bio</code>，所有的<code>bios[]</code>来自复制的这份<blockquote>
<p>没有设置behind模式，阵列所有成员盘写命令完成后，这个写请求才算完成，释放上层BIO；如果设置了behind模式且存在<code>WriteMostly</code>盘，那么除<code>WriteMostly</code>外其他盘写完成则认为写请求完成，释放上层BIO；<code>behind_master_bio</code>是阵列所有盘写完之后释放的</p>
</blockquote>
</li>
<li>组装<code>bios[]</code>上的BIO</li>
<li>增加未提交命令计数<code>remaining</code></li>
<li>当前线程正在进行BIO的plug，则将当前产生的<code>mbio</code>插入到<code>plug-&gt;pending</code><blockquote>
<p><code>blk_finish_plug</code>中会对这里BIO进行处理，不用重新唤醒处理线程，提升性能<br>5.当前进程没有plug，则将<code>mbio</code>插入到<code>pending_bio_list</code>上，然后唤醒守护进程处理</p>
</blockquote>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">r1_bio_write_done(r1_bio);</span><br><span class="line"></span><br><span class="line">/* In case raid1d snuck in to freeze_array */</span><br><span class="line">wake_up(&amp;conf-&gt;wait_barrier);</span><br></pre></td></tr></table></figure>
<p>判断写完成处理以及唤醒等待写屏障的流程</p>
<h2><span id="flush_pending_writes">flush_pending_writes</span></h2><p>该函数为写请求下发，由守护进程发起</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static void flush_pending_writes(struct r1conf *conf)</span><br><span class="line">&#123;</span><br><span class="line">	/* Any writes that have been queued but are awaiting</span><br><span class="line">	 * bitmap updates get flushed here.</span><br><span class="line">	 */</span><br><span class="line">	spin_lock_irq(&amp;conf-&gt;device_lock);</span><br><span class="line"></span><br><span class="line">	if (conf-&gt;pending_bio_list.head) &#123;</span><br><span class="line">		struct blk_plug plug;</span><br><span class="line">		struct bio *bio;</span><br><span class="line"></span><br><span class="line">		bio = bio_list_get(&amp;conf-&gt;pending_bio_list);</span><br><span class="line">		conf-&gt;pending_count = 0;</span><br><span class="line">		spin_unlock_irq(&amp;conf-&gt;device_lock);</span><br><span class="line"></span><br><span class="line">		/*</span><br><span class="line">		 * As this is called in a wait_event() loop (see freeze_array),</span><br><span class="line">		 * current-&gt;state might be TASK_UNINTERRUPTIBLE which will</span><br><span class="line">		 * cause a warning when we prepare to wait again.  As it is</span><br><span class="line">		 * rare that this path is taken, it is perfectly safe to force</span><br><span class="line">		 * us to go around the wait_event() loop again, so the warning</span><br><span class="line">		 * is a false-positive.  Silence the warning by resetting</span><br><span class="line">		 * thread state</span><br><span class="line">		 */</span><br><span class="line">		__set_current_state(TASK_RUNNING);</span><br><span class="line">		blk_start_plug(&amp;plug);</span><br><span class="line">		flush_bio_list(conf, bio);</span><br><span class="line">		blk_finish_plug(&amp;plug);</span><br><span class="line">	&#125; else</span><br><span class="line">		spin_unlock_irq(&amp;conf-&gt;device_lock);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li><code>pending_bio_list</code>上的所有请求一次性下发到盘，同时<code>pending_count</code>计数清零</li>
<li><code>blk_finish_plug</code>会处理插入到<code>plug-&gt;pending</code>中的请求</li>
</ol>
<h2><span id="raid1_end_write_request">raid1_end_write_request</span></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (behind) &#123;</span><br><span class="line">	if (test_bit(WriteMostly, &amp;rdev-&gt;flags))</span><br><span class="line">		atomic_dec(&amp;r1_bio-&gt;behind_remaining);</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * In behind mode, we ACK the master bio once the I/O</span><br><span class="line">	 * has safely reached all non-writemostly</span><br><span class="line">	 * disks. Setting the Returned bit ensures that this</span><br><span class="line">	 * gets done only once -- we don&#x27;t ever want to return</span><br><span class="line">	 * -EIO here, instead we&#x27;ll wait</span><br><span class="line">	 */</span><br><span class="line">	if (atomic_read(&amp;r1_bio-&gt;behind_remaining) &gt;= (atomic_read(&amp;r1_bio-&gt;remaining)-1) &amp;&amp;</span><br><span class="line">	    test_bit(R1BIO_Uptodate, &amp;r1_bio-&gt;state)) &#123;</span><br><span class="line">		/* Maybe we can return now */</span><br><span class="line">		if (!test_and_set_bit(R1BIO_Returned, &amp;r1_bio-&gt;state)) &#123;</span><br><span class="line">			struct bio *mbio = r1_bio-&gt;master_bio;</span><br><span class="line">			pr_debug(&quot;raid1: behind end write sectors&quot;</span><br><span class="line">				 &quot; %llu-%llu\n&quot;,</span><br><span class="line">				 (unsigned long long) mbio-&gt;bi_iter.bi_sector,</span><br><span class="line">				 (unsigned long long) bio_end_sector(mbio) - 1);</span><br><span class="line">			call_bio_endio(r1_bio);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>behind模式下判断所有非<code>WriteMostly</code>盘完成命令后通知上层写请求结束<code>call_bio_endio</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Let&#x27;s see if all mirrored write operations have finished</span><br><span class="line"> * already.</span><br><span class="line"> */</span><br><span class="line">r1_bio_write_done(r1_bio);</span><br><span class="line"></span><br><span class="line">if (to_put)</span><br><span class="line">	bio_put(to_put);</span><br></pre></td></tr></table></figure>
<p>正常模式或是behind模式下所有写请求完成后的处理（释放相关资源、bitmap处理）</p>
<h1><span id="总体流程">总体流程</span></h1><p>参考<a href="https://www.cnblogs.com/fangpei/p/4676395.html">fangpei</a>作者总结</p>
<h1><span id="参考">参考</span></h1><ul>
<li><a href="https://www.cnblogs.com/fangpei/p/4676395.html">https://www.cnblogs.com/fangpei/p/4676395.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>MD</tag>
      </tags>
  </entry>
  <entry>
    <title>RAID1源码分析~读.md</title>
    <url>/2022/05/21/RAID1%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E8%AF%BB/</url>
    <content><![CDATA[<h1><span id="源码分析">源码分析</span></h1><p>基于4.19.220</p>
<p>读操作比较简单，我们主要分析一下读目标盘的选择算法<code>read_balance</code>的实现</p>
<h2><span id="read_balance">read_balance</span></h2><p>目标盘选择的依据如下：</p>
<ol>
<li>首先选择上次操作结束位置是本次操作的起点;</li>
<li>如果不存在这样的盘，如果盘阵中有SSD盘或是有从未访问过得盘那么选择这块盘；</li>
<li>否则选择上次操作结束位置离 本次起始位置最近的盘。</li>
<li>如果盘阵中都是<code>WriteMostly</code>盘，则选择遍历中的第一块<code>WriteMostly</code>盘。</li>
</ol>
<span id="more"></span>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if ((conf-&gt;mddev-&gt;recovery_cp &lt; this_sector + sectors) ||</span><br><span class="line">	    (mddev_is_clustered(conf-&gt;mddev) &amp;&amp;</span><br><span class="line">	    md_cluster_ops-&gt;area_resyncing(conf-&gt;mddev, READ, this_sector,</span><br><span class="line">		    this_sector + sectors)))</span><br><span class="line">		choose_first = 1;</span><br><span class="line">	else</span><br><span class="line">		choose_first = 0;</span><br></pre></td></tr></table></figure>
<p>如果读的起始位置大于同步窗口则选择第一块可读盘作为目标盘（未同步数据可能是脏数据，因此读哪块都可以）；如果盘阵同步不在运行，或是读的其实位置小于同步数据位置，那么盘阵中所有的盘都有可能被选为目标读盘</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for (disk = 0 ; disk &lt; conf-&gt;raid_disks * 2 ; disk++) &#123;</span><br><span class="line">	sector_t dist;</span><br><span class="line">	sector_t first_bad;</span><br><span class="line">	int bad_sectors;</span><br><span class="line">	unsigned int pending;</span><br><span class="line">	bool nonrot;</span><br></pre></td></tr></table></figure>
<p>遍历盘阵中的盘，找到符合<code>read_balance</code>条件的盘</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rdev = rcu_dereference(conf-&gt;mirrors[disk].rdev);</span><br><span class="line">if (r1_bio-&gt;bios[disk] == IO_BLOCKED</span><br><span class="line">    || rdev == NULL</span><br><span class="line">    || test_bit(Faulty, &amp;rdev-&gt;flags))</span><br><span class="line">	continue;</span><br><span class="line">if (!test_bit(In_sync, &amp;rdev-&gt;flags) &amp;&amp;</span><br><span class="line">    rdev-&gt;recovery_offset &lt; this_sector + sectors)</span><br><span class="line">	continue;</span><br><span class="line">if (test_bit(WriteMostly, &amp;rdev-&gt;flags)) &#123;</span><br><span class="line">	/* Don&#x27;t balance among write-mostly, just</span><br><span class="line">	 * use the first as a last resort */</span><br><span class="line">	if (best_dist_disk &lt; 0) &#123;</span><br><span class="line">		if (is_badblock(rdev, this_sector, sectors,</span><br><span class="line">				&amp;first_bad, &amp;bad_sectors)) &#123;</span><br><span class="line">			if (first_bad &lt;= this_sector)</span><br><span class="line">				/* Cannot use this */</span><br><span class="line">				continue;</span><br><span class="line">			best_good_sectors = first_bad - this_sector;</span><br><span class="line">		&#125; else</span><br><span class="line">			best_good_sectors = sectors;</span><br><span class="line">		best_dist_disk = disk;</span><br><span class="line">		best_pending_disk = disk;</span><br><span class="line">	&#125;</span><br><span class="line">	continue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>跳过不存在及坏盘（<code>IO_BLOCKED</code>待分析）</li>
<li>如果盘未就绪且同步恢复的位置小于读的起始位置则跳过（未就绪的盘且同步恢复的位置大于读起始位置可读，<em>原因待分析</em>）</li>
<li>如果是<code>WriteMostly</code>设置<code>best_dist_disk</code>及<code>best_pending_disk</code>（只设置一次，如果盘阵全部是这种盘，则使用第一次扫描到的<code>WriteMostly</code>盘）<blockquote>
<p>对于坏块处理，如果存在坏块且坏块起始位置小于读的位置则不能作为读盘（读的位置存在坏块），否则计算出读的位置到坏块的实际可用的位置</p>
</blockquote>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* This is a reasonable device to use.  It might</span><br><span class="line"> * even be best.</span><br><span class="line"> */</span><br><span class="line">if (is_badblock(rdev, this_sector, sectors,</span><br><span class="line">		&amp;first_bad, &amp;bad_sectors)) &#123;</span><br><span class="line">	if (best_dist &lt; MaxSector)</span><br><span class="line">		/* already have a better device */</span><br><span class="line">		continue;</span><br><span class="line">	if (first_bad &lt;= this_sector) &#123;</span><br><span class="line">		/* cannot read here. If this is the &#x27;primary&#x27;</span><br><span class="line">		 * device, then we must not read beyond</span><br><span class="line">		 * bad_sectors from another device..</span><br><span class="line">		 */</span><br><span class="line">		bad_sectors -= (this_sector - first_bad);</span><br><span class="line">		if (choose_first &amp;&amp; sectors &gt; bad_sectors)</span><br><span class="line">			sectors = bad_sectors;</span><br><span class="line">		if (best_good_sectors &gt; sectors)</span><br><span class="line">			best_good_sectors = sectors;</span><br><span class="line"></span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		sector_t good_sectors = first_bad - this_sector;</span><br><span class="line">		if (good_sectors &gt; best_good_sectors) &#123;</span><br><span class="line">			best_good_sectors = good_sectors;</span><br><span class="line">			best_disk = disk;</span><br><span class="line">		&#125;</span><br><span class="line">		if (choose_first)</span><br><span class="line">			break;</span><br><span class="line">	&#125;</span><br><span class="line">	continue;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">	if ((sectors &gt; best_good_sectors) &amp;&amp; (best_disk &gt;= 0))</span><br><span class="line">		best_disk = -1;</span><br><span class="line">	best_good_sectors = sectors;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>盘存在坏块是做如下处理：</p>
<ol>
<li>盘存在坏块，但是前面遍历有可用的盘（<code>best_dist &lt; MaxSector</code>）,则忽略这个坏块盘继续遍历下一块盘</li>
<li>如果坏块起始位置小于等于读的位置，那么这块盘是不可读，并且记录这块盘从读的起始位置有多少坏块（如果后续有可读盘，这个被丢弃<code>best_good_sectors</code>重新被赋值）</li>
<li>如果坏块起始位置大于读的位置，那么从读的起始位置到坏块的位置这块区间是可读的<blockquote>
<p><code>good_sectors &gt; best_good_sectors</code>符合这个条件情况</p>
<ol>
<li><code>best_good_sectors == 0</code>这块盘是遍历的第一块盘</li>
<li><code>best_good_sectors != 0 &amp;&amp; good_sectors &gt; best_good_sectors</code>已经遍历过存在坏块的盘，但是这块盘可读的数据优于上一个遍历的坏块盘</li>
<li>否则不选用这块盘<br>对于不存在换块的处理，如果前面选择了有坏块的盘作为读盘则取消，重新设置<code>best_good_sectors</code>值</li>
</ol>
</blockquote>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nonrot = blk_queue_nonrot(bdev_get_queue(rdev-&gt;bdev));</span><br><span class="line">has_nonrot_disk |= nonrot;</span><br><span class="line">pending = atomic_read(&amp;rdev-&gt;nr_pending);</span><br><span class="line">dist = abs(this_sector - conf-&gt;mirrors[disk].head_position);</span><br><span class="line">if (choose_first) &#123;</span><br><span class="line">	best_disk = disk;</span><br><span class="line">	break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>获取盘的类型、IO下发次数以及上次操作结束位置到本次读写位置的距离</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* Don&#x27;t change to another disk for sequential reads */</span><br><span class="line">if (conf-&gt;mirrors[disk].next_seq_sect == this_sector</span><br><span class="line">    || dist == 0) &#123;</span><br><span class="line">	int opt_iosize = bdev_io_opt(rdev-&gt;bdev) &gt;&gt; 9;</span><br><span class="line">	struct raid1_info *mirror = &amp;conf-&gt;mirrors[disk];</span><br><span class="line"></span><br><span class="line">	best_disk = disk;</span><br><span class="line">	/*</span><br><span class="line">	 * If buffered sequential IO size exceeds optimal</span><br><span class="line">	 * iosize, check if there is idle disk. If yes, choose</span><br><span class="line">	 * the idle disk. read_balance could already choose an</span><br><span class="line">	 * idle disk before noticing it&#x27;s a sequential IO in</span><br><span class="line">	 * this disk. This doesn&#x27;t matter because this disk</span><br><span class="line">	 * will idle, next time it will be utilized after the</span><br><span class="line">	 * first disk has IO size exceeds optimal iosize. In</span><br><span class="line">	 * this way, iosize of the first disk will be optimal</span><br><span class="line">	 * iosize at least. iosize of the second disk might be</span><br><span class="line">	 * small, but not a big deal since when the second disk</span><br><span class="line">	 * starts IO, the first disk is likely still busy.</span><br><span class="line">	 */</span><br><span class="line">	if (nonrot &amp;&amp; opt_iosize &gt; 0 &amp;&amp;</span><br><span class="line">	    mirror-&gt;seq_start != MaxSector &amp;&amp;</span><br><span class="line">	    mirror-&gt;next_seq_sect &gt; opt_iosize &amp;&amp;</span><br><span class="line">	    mirror-&gt;next_seq_sect - opt_iosize &gt;=</span><br><span class="line">	    mirror-&gt;seq_start) &#123;</span><br><span class="line">		choose_next_idle = 1;</span><br><span class="line">		continue;</span><br><span class="line">	&#125;</span><br><span class="line">	break;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if (choose_next_idle)</span><br><span class="line">	continue;</span><br></pre></td></tr></table></figure>
<p>如果上次操作的结束位置和本次读写的起始位置相同，则选择这块盘</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (min_pending &gt; pending) &#123;</span><br><span class="line">	min_pending = pending;</span><br><span class="line">	best_pending_disk = disk;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if (dist &lt; best_dist) &#123;</span><br><span class="line">	best_dist = dist;</span><br><span class="line">	best_dist_disk = disk;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>计算盘读写的最小次数以及操作最近的距离</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * If all disks are rotational, choose the closest disk. If any disk is</span><br><span class="line"> * non-rotational, choose the disk with less pending request even the</span><br><span class="line"> * disk is rotational, which might/might not be optimal for raids with</span><br><span class="line"> * mixed ratation/non-rotational disks depending on workload.</span><br><span class="line"> */</span><br><span class="line">if (best_disk == -1) &#123;</span><br><span class="line">	if (has_nonrot_disk || min_pending == 0)</span><br><span class="line">		best_disk = best_pending_disk;</span><br><span class="line">	else</span><br><span class="line">		best_disk = best_dist_disk;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>遍历完后，如果没有找到想要的盘，则判断有SSD或是没有读写的盘，则选用这块盘；否则选用位置最近的盘</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (best_disk &gt;= 0) &#123;</span><br><span class="line">	rdev = rcu_dereference(conf-&gt;mirrors[best_disk].rdev);</span><br><span class="line">	if (!rdev)</span><br><span class="line">		goto retry;</span><br><span class="line">	atomic_inc(&amp;rdev-&gt;nr_pending);</span><br><span class="line">	sectors = best_good_sectors;</span><br><span class="line"></span><br><span class="line">	if (conf-&gt;mirrors[best_disk].next_seq_sect != this_sector)</span><br><span class="line">		conf-&gt;mirrors[best_disk].seq_start = this_sector;</span><br><span class="line"></span><br><span class="line">	conf-&gt;mirrors[best_disk].next_seq_sect = this_sector + sectors;</span><br><span class="line">&#125;</span><br><span class="line">rcu_read_unlock();</span><br><span class="line">*max_sectors = sectors;</span><br></pre></td></tr></table></figure>
<p>选择该盘，更新读写位置</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>MD</tag>
      </tags>
  </entry>
</search>
